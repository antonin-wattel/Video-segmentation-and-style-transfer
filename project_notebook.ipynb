{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Processing with Object Segmentation and Style Transfer\n",
    "\n",
    "Antonin Wattel, Pierre Pages, 12/2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import importlib.util\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from IPython.display import Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#current working directory (this should be the base directory of the project)\n",
    "CWD = os.getcwd()\n",
    "\n",
    "#to import modules\n",
    "def module_from_file(module_name, file_path):\n",
    "    spec = importlib.util.spec_from_file_location(module_name, file_path)\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(module)\n",
    "    return module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Video Choice\n",
    "The first thing to do is to choose out video to be processed. The longer it lasts, and the greater the resolution is, the longer the process will take."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom choice \n",
    "\n",
    "If you want to use a preprocesed video, skip this part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose an mp4 video and place it in the unprocessed_videos folder\n",
    "#set seq_name to the video name (without .mp4 extension)\n",
    "#avoid spaces in the video name\n",
    "\n",
    "seq_name = 'skate_sage_elsesser'\n",
    "video_path = os.path.join(CWD, 'unprocessed_videos/{}.mp4'.format(seq_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize video\n",
    "Video('unprocessed_videos/{}.mp4'.format(seq_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trimming the video \n",
    "\n",
    "The longer the video is, the longer the process, so keeping a video of only a few seconds is good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose timecodes for trimming\n",
    "time1 = '00:00:17.8'\n",
    "time2 = '00:00:24'\n",
    "output_path = os.path.join(CWD, 'unprocessed_videos', '{}_trimmed.mp4'.format(seq_name))\n",
    "#command = '! ffmpeg -i {} -ss {} -to {} -c copy {} -y'.format(video_path, time1, time2, output_path)\n",
    "command = '! ffmpeg -i {} -ss {} -to {} {} -y'.format(video_path, time1, time2, output_path)\n",
    "print(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "#paste the above output and run\n",
    "! <insert command here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize trimmed video\n",
    "Video('unprocessed_videos/{}_trimmed.mp4'.format(seq_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion to image sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now convert the mp4 into a sequence of images, and put this sequence in the appropriate folder, ready to be processed\n",
    "input_video = output_path\n",
    "output_sequence = os.path.join(CWD, 'OSVOS-PyTorch/DAVIS-data/DAVIS/JPEGImages/480p/{}'.format(seq_name))\n",
    "if not os.path.exists(output_sequence):\n",
    "    os.makedirs(output_sequence)\n",
    "\n",
    "command = '! ffmpeg  -i {} -qscale:v 2 -start_number 0 {}/%05d.jpg -y'.format(input_video, output_sequence)\n",
    "print(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "#paste the above output and run\n",
    "! <insert command here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking\n",
    "\n",
    "Now that we have the sequence of images, we need to have a mask of the object we want to segment for the first image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Method 1 : use external software "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manually create the mask of the first image of the sequence with a software (photoshop, gimp...) and save it as 00000.png in the followig folder\n",
    "\n",
    "mask_path = os.path.join(CWD, 'OSVOS-PyTorch/DAVIS-data/DAVIS/Annotations/480p/{}'.format(seq_name))\n",
    "if not os.path.exists(mask_path):\n",
    "    os.makedirs(mask_path)\n",
    "\n",
    "print(mask_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check it looks good\n",
    "plt.figure(figsize =(20, 10))\n",
    "background_image = cv2.imread(os.path.join(output_sequence, '00000.jpg'))\n",
    "background_image = cv2.cvtColor(background_image, cv2.COLOR_BGR2RGB)\n",
    "mask_image = cv2.imread(os.path.join(mask_path, '00000.png'))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(background_image)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(mask_image)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Method 2 : Foreground extraction using GrabCut Algorithm\n",
    "\n",
    "https://docs.opencv.org/4.x/d8/d83/tutorial_py_grabcut.html\n",
    "\n",
    "to be improved by manually marking the mask image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#play with the reclangle coordinates to have a good mask\n",
    "#(rq: this will work well will well defined/contrasted foreground/background)\n",
    "\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "img =  cv2.imread(os.path.join(output_sequence, '00000.jpg'))\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "mask = np.zeros(img.shape[:2],np.uint8)\n",
    "\n",
    "bgdModel = np.zeros((1,65),np.float64)\n",
    "fgdModel = np.zeros((1,65),np.float64)\n",
    "\n",
    "rect = (430,120,260,320)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (20, 20))\n",
    "plt.gca().add_patch(Rectangle((rect[0], rect[1]), rect[2], rect[3], edgecolor='red', facecolor='none', lw=1))\n",
    "\n",
    "cv2.grabCut(img,mask,rect,bgdModel,fgdModel,5,cv2.GC_INIT_WITH_RECT)\n",
    "\n",
    "mask2 = np.where((mask==2)|(mask==0),0,1).astype('uint8')\n",
    "img = img*mask2[:,:,np.newaxis]*1000\n",
    "\n",
    "plt.xticks([])  \n",
    "plt.yticks([])  \n",
    "\n",
    "\n",
    "ax.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessed video\n",
    "\n",
    "The DAVIS dataset already provides short videos put into sequences of images, as well as the associated masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can use preprocessed videos from the validation set (choose among the following)\n",
    "\n",
    "# blackswan, bmx-trees, breakdance, camel, car-roundabout,\n",
    "# car-shadow, cows, dance-twirl, dog, drift-chicane, \n",
    "# drift-straight, goat, horsejump-high, kite-surf,\n",
    "# libby, motocross-jump, paragliding-launch, parkour, \n",
    "# scooter-black, soapbox\n",
    "\n",
    "seq_name = 'blackswan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make video out of sequence (for demonstration)\n",
    "\n",
    "data_path = 'OSVOS-PyTorch/DAVIS-data/DAVIS/JPEGImages/480p/{} '.format(seq_name)\n",
    "out_path = os.path.join(CWD, 'Results/in_videos/{}.mp4'.format(seq_name))\n",
    "if not os.path.exists(out_path):\n",
    "    os.makedirs(out_path)\n",
    "\n",
    "command = '! cd {} & ffmpeg -r 25 -f image2 -s 720*480 -i %05d.jpg -vcodec libx264 -crf 25  -pix_fmt yuv420p -y {}'.format(data_path, out_path)\n",
    "print(command)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "#paste the above output and run\n",
    "! cd OSVOS-PyTorch/DAVIS-data/DAVIS/JPEGImages/480p/blackswan  & ffmpeg -r 25 -f image2 -s 720*480 -i %05d.jpg -vcodec libx264 -crf 25  -pix_fmt yuv420p -y c:\\Users\\A1234\\Documents\\INF573\\project\\Results/in_videos/blackswan.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Video('Results/in_videos/{}.mp4'.format(seq_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Object segmentation\n",
    "\n",
    "The object segmentaion is based on the PyTorch implementation of One-Shot Video Object Segmentation (OSVOS)\n",
    "https://github.com/kmaninis/OSVOS-PyTorch.\n",
    "<br/>\n",
    "We use a pretrained parent model, available at https://data.vision.ee.ethz.ch/kmaninis/share/OSVOS/Downloads/models/pth_parent_model.zip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import from other files\n",
    "sys.path.append(os.path.join(CWD, 'OSVOS-Pytorch'))\n",
    "train_path = os.path.join(CWD, 'OSVOS-Pytorch', 'train_online.py')\n",
    "train_online = module_from_file(\"train_online\", train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a few training parameters\n",
    "#for more parameters, see OSVOS-Pytorch/train_online.py\n",
    "\n",
    "seq_name=seq_name\n",
    "save_dir_res= os.path.join(CWD, 'Results', 'masks', seq_name, 'sequences')\n",
    "nAveGrad=5\n",
    "nEpochs= 300 * nAveGrad #the more epochs we take, the better the segementation results\n",
    "\n",
    "train_online.train(seq_name, save_dir_res, nAveGrad, nEpochs)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_path = 'Results/masks/{}/sequences'.format(seq_name)\n",
    "out_path = os.path.join(CWD, 'Results/in_videos/{}.mp4'.format(seq_name))\n",
    "if not os.path.exists(out_path):\n",
    "    os.makedirs(out_path)\n",
    "command = '! cd {} & ffmpeg -r 25 -f image2 -s 720*480 -i %05d.png -vcodec libx264 -crf 25  -pix_fmt yuv420p -y mask_video.mp4'.format(res_path)\n",
    "print(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "#paste the above output and run\n",
    "! <insert command here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(\"Results/masks/{}/sequences/mask_video.mp4\".format(seq_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Style transfer\n",
    "\n",
    "This part is based on the PyTorch implementation https://github.com/rrmina/fast-neural-style-pytorch following the style transfer approach outlined in Perceptual Losses for Real-Time Style Transfer and Super-Resolution paper by Justin Johnson, Alexandre Alahi, and Fei-Fei Li, along with the supplementary paper detailing the exact model architecture of the mentioned paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import files\n",
    "sys.path.append(os.path.join(CWD, 'fast-neural-style-pytorch'))\n",
    "\n",
    "train_path = os.path.join(CWD, 'fast-neural-style-pytorch', 'train.py')\n",
    "style_train = module_from_file(\"train\", train_path)\n",
    "\n",
    "stylize_path = os.path.join(CWD, 'fast-neural-style-pytorch', 'stylize.py')\n",
    "stylize = module_from_file(\"stylize\", stylize_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training style transfer network\n",
    "This step will take some time to run.\n",
    "You can alternatively use pretrained models (see next step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the models on a portion (2000 images) of the coco2014 dataset, available at https://cocodataset.org/#download.<br/>\n",
    "The training dataset must be placed in the folder fast-neural-style-pytorch/data. </br>\n",
    "<br/>\n",
    "This model is trained on the pretrained vgg model available at https://web.eecs.umich.edu/~justincj/models/vgg16-00b39a1b.pth. <br/>\n",
    "This model must be placed in fast-neural-style-pytorch/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use a TransformerNetwrok (see transformer.py for architecture).\n",
    "Some other experimental transformer architectures can be tested (see experimental.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Choice of style image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose an image to train the style on\n",
    "#in the folder images, we provide a few images for the style\n",
    "#To choose your own, choose a jpg image and place it in the fast-neural-style-pytorch/images folder\n",
    "#careful: too large images will lead to long training times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_image_name = 'picabia' # (this is an image made with 'Vision of Chaos' software)\n",
    "style_image_path = os.path.join(CWD, 'fast-neural-style-pytorch', 'images', '{}.jpg'.format(style_image_name))\n",
    "\n",
    "style_image = cv2.imread(style_image_path)\n",
    "style_image = cv2.cvtColor(style_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(style_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#200 iterations ->\n",
    "\n",
    "num_epochs = 100\n",
    "dataset_path = os.path.join(CWD, 'fast-neural-style-pytorch', 'data', 'train2014' )#to fill\n",
    "style_image_path = os.path.join(CWD, 'fast-neural-style-pytorch', 'images', '{}.jpg'.format(style_image_name))\n",
    "save_checkpoints_path = os.path.join(CWD, 'fast-neural-style-pytorch', 'models')\n",
    "save_model_path_final = os.path.join(CWD, 'fast-neural-style-pytorch', 'transforms', '{}.pth'.format(style_image_name))\n",
    "save_image_path = os.path.join(CWD, 'fast-neural-style-pytorch', 'images/out')\n",
    "save_model_every = 10\n",
    "plt_loss = 1\n",
    "show_images = 1\n",
    "\n",
    "args = [num_epochs, dataset_path, style_image_path, save_checkpoints_path, save_model_path_final, save_image_path, save_model_every, plt_loss, show_images]\n",
    "style_train.train(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained style transfer models\n",
    "You can choose among the follwowing pretrained styles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#see README inside fast-neural-style-pytorch/transforms for more pretrained + experimental models\n",
    "\n",
    "# As seen on https://github.com/rrmina/fast-neural-style-pytorch\n",
    "# bayanihan, lazy, mosaic, starry, \n",
    "# tokyo_ghoul, udnie, wave, mosaic_TransformerResNetwork\n",
    "\n",
    "# I also trained the following\n",
    "# multicolor\n",
    "# picabia\n",
    "# pointillism\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this cell to use a pretrained model of your choice\n",
    "style_image_name = 'wave'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Styling videos\n",
    "We style images one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_model_foreground =  None\n",
    "style_model_background = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Foreground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_model_foreground = 'wave'\n",
    "style_path_foreground = os.path.join(CWD, 'fast-neural-style-pytorch/transforms/{}.pth'.format(style_model_foreground))\n",
    "preserve_color = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_folder = os.path.join(CWD, 'OSVOS-PyTorch/DAVIS-data/DAVIS/JPEGImages/480p/{}'.format(seq_name))\n",
    "save_folder = os.path.join(CWD,'Results/stylized/{}/{}'.format(seq_name, style_model_foreground))\n",
    "#create the folders if they don't exist \n",
    "\n",
    "if not os.path.exists(save_folder):\n",
    "    os.makedirs(save_folder)\n",
    "\n",
    "if style_model_foreground is not None:\n",
    "    stylize.stylize_folder_single(style_path_foreground, content_folder , save_folder, preserve_color)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save and play the video\n",
    "\n",
    "if style_model_foreground is not None:\n",
    "    res_path = save_folder\n",
    "    command = '! cd {} & ffmpeg -r 25 -f image2 -s 720*480 -i %05d.jpg -vcodec libx264 -crf 25  -pix_fmt yuv420p -y video.mp4'.format(res_path)\n",
    "    print(command)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "#paste previous line here\n",
    "! <insert command here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_model_background = 'multicolor'\n",
    "style_path_foreground = os.path.join(CWD, 'fast-neural-style-pytorch/transforms/{}.pth'.format(style_model_background))\n",
    "preserve_color = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_folder = os.path.join(CWD, 'OSVOS-PyTorch/DAVIS-data/DAVIS/JPEGImages/480p/{}'.format(seq_name))\n",
    "save_folder = os.path.join(CWD,'Results/stylized/{}/{}'.format(seq_name, style_model_background))\n",
    "#create the folders if they don't exist \n",
    "if not os.path.exists(save_folder):\n",
    "    os.makedirs(save_folder)\n",
    "\n",
    "if style_model_background is not None:\n",
    "    stylize.stylize_folder_single(style_path_foreground, content_folder , save_folder, preserve_color)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save and play the video\n",
    "\n",
    "if style_model_background is not None:\n",
    "    res_path = save_folder\n",
    "    command = '! cd {} & ffmpeg -r 25 -f image2 -s 720*480 -i %05d.jpg -vcodec libx264 -crf 25  -pix_fmt yuv420p -y video.mp4'.format(res_path)\n",
    "    print(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "#paste previous line here\n",
    "! cd c:\\Users\\A1234\\Documents\\INF573\\project\\Results/stylized/skate_sage_elsesser/multicolor & ffmpeg -r 25 -f image2 -s 720*480 -i %05d.jpg -vcodec libx264 -crf 25  -pix_fmt yuv420p -y video.mp4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Merging videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e175413f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def mix_images(background_image_path, mask_path, foreground_path, out, mask_only):\n",
    "    \n",
    "    background_image = cv2.imread(background_image_path)\n",
    "    background_image = background_image.astype(float)#[:, :-2, :] \n",
    "    print('background: ', background_image.shape)\n",
    "\n",
    "    mask  = cv2.imread(mask_path) \n",
    "    print('mask: ', mask.shape)\n",
    "    mask = cv2.GaussianBlur(mask, (3, 3), 0)\n",
    "    mask = mask.astype(float)/255\n",
    "\n",
    "    foreground  = cv2.imread(foreground_path) \n",
    "    foreground = foreground.astype(float)[:, :-2, :] #careful with dimensions\n",
    "    print('foreground: ', foreground.shape)\n",
    "    \n",
    "    tmp1 =  cv2.multiply(mask, foreground)\n",
    "    tmp2 = cv2.multiply(1.0 - mask, background_image)\n",
    "\n",
    "    if mask_only:\n",
    "        out_image = tmp1\n",
    "    else:\n",
    "        outImage = cv2.add(tmp1, tmp2)\n",
    "\n",
    "    cv2.imwrite(out, outImage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a16ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_images_batch(background_path, mask_path, foreground_path, out_folder, mask_only=False):\n",
    "    \n",
    "    images = [img for img in os.listdir(background_path) if img.endswith(\".jpg\") or img.endswith(\".png\")]\n",
    "    print(images)\n",
    "    for image_name in images:\n",
    "        background = os.path.join(background_path, image_name)\n",
    "        print(background)\n",
    "        foreground = os.path.join(foreground_path, image_name)\n",
    "        print(foreground)\n",
    "        mask = os.path.join(mask_path, image_name[:-4]+'.png')\n",
    "        print(mask)\n",
    "        #print('mask:', mask)\n",
    "        out = os.path.join(out_folder, image_name[:-4]+'.png')\n",
    "\n",
    "        mix_images(background, mask, foreground, out, mask_only)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0345e7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_path =  os.path.join(CWD, 'OSVOS-PyTorch/DAVIS-data/DAVIS/JPEGImages/480p/{}'.format(seq_name))\n",
    "background_path = original_path\n",
    "foreground_path = original_path\n",
    "#style_model_foreground = None\n",
    "style_model_background = None\n",
    "\n",
    "\n",
    "if style_model_background is not None:\n",
    "    background_path = os.path.join(CWD,'Results/stylized/{}/{}'.format(seq_name, style_model_background))\n",
    "\n",
    "if style_model_foreground is not None:\n",
    "    foreground_path = os.path.join(CWD,'Results/stylized/{}/{}'.format(seq_name, style_model_foreground))\n",
    "\n",
    "mask_path = os.path.join(CWD, 'Results/masks/{}/sequences'.format(seq_name))\n",
    "out_path = os.path.join(CWD, 'Results/final/{}/b_{}+f_{}/sequences'.format(seq_name, style_model_background, style_model_foreground)) #might want to add the background and foreground name\n",
    "\n",
    "if not os.path.exists(out_path):\n",
    "    os.makedirs(out_path)\n",
    "\n",
    "mix_images_batch(background_path, mask_path, foreground_path, out_path)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = '! cd {} & ffmpeg -r 25 -f image2 -s 720*480 -i %05d.png -vcodec libx264 -crf 25  -pix_fmt yuv420p -y final_video.mp4'.format(out_path)\n",
    "print(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "#paste the above output and run\n",
    "! <insert command here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "Video('Results/final/{}/b_{}+f_{}/sequences/final_video.mp4'.format(seq_name, style_model_background, style_model_foreground))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5aca2a9dbfa0d60b06e27551eb7c71695518b7fe593f7fe648890592feaeeb3d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('osvos_env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
